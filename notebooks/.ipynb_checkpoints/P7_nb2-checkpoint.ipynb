{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd63dab9-23d6-4509-a8c3-6b87596aa152",
   "metadata": {},
   "source": [
    "# temps de debut execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87c6f73d-9745-4ae1-926e-9ea4b972781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exécution du notebook commencée à: 2024-06-21 16:29:17.857682\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "# Marquer le début de l'exécution\n",
    "start_time = time.time()\n",
    "print(f\"Exécution du notebook commencée à: {datetime.datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cc966-809d-441f-9d1b-f01fa7e9f526",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861bfab-0697-4663-9595-40cb7b2411aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=  12.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=100; total time=  51.8s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=  17.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=200; total time=  26.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=100; total time=  44.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   6.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=300; total time=  16.7s\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=100; total time=  14.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=100; total time=  39.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=100; total time=  12.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=100; total time=  45.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=  13.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=100; total time=  14.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=300; total time=  39.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=300; total time=  59.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=300; total time=  45.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   4.9s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=200; total time=  29.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=300; total time= 2.0min\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  13.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=100; total time=  16.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=300; total time=  45.0s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   9.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=300; total time=  47.7s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   7.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=  12.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=100; total time=  17.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=100; total time=  31.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=300; total time= 1.7min\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   9.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=300; total time=  48.0s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   7.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=  18.7s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=200; total time=  31.6s\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=200; total time= 1.2min\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=100; total time=  40.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=100; total time=  12.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=100; total time=  53.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=300; total time=  18.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=200; total time=  31.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=200; total time= 1.3min\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=200; total time=  40.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=  12.9s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=200; total time= 1.3min\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=300; total time=  46.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=300; total time= 1.7min\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   5.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=200; total time=  32.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=300; total time= 2.4min\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=200; total time=  29.8s\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=300; total time=  32.7s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   5.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=200; total time=  29.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=300; total time= 2.4min\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=200; total time=  22.8s\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=200; total time=  48.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=100; total time=  12.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=200; total time= 1.6min\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=200; total time=  50.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=300; total time=  17.2s\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=200; total time=  28.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=200; total time=  45.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=300; total time=  13.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=9, n_estimators=200; total time= 1.6min\n",
      "[CV] END ...learning_rate=0.1, max_depth=9, n_estimators=100; total time=  45.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  12.5s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=300; total time=  20.8s\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=100; total time=  26.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=300; total time=  43.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   8.9s\n",
      "[CV] END ..learning_rate=0.01, max_depth=6, n_estimators=300; total time=  47.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   7.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=  12.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=100; total time=  17.7s\n",
      "[CV] END ...learning_rate=0.1, max_depth=6, n_estimators=300; total time=  45.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   8.8s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=100; total time=   8.1s\n",
      "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  12.4s\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=100; total time=  15.6s\n",
      "[CV] END ...learning_rate=0.2, max_depth=6, n_estimators=300; total time=  39.0s\n",
      "[CV] END ...learning_rate=0.2, max_depth=9, n_estimators=300; total time=  37.8s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Configurer MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Credit Risk Model\")\n",
    "\n",
    "# Charger le DataFrame\n",
    "app_datas = pd.read_csv('../csv_files/app_datas_light.csv')\n",
    "\n",
    "# Mode débogage\n",
    "debug_mode = True\n",
    "\n",
    "if debug_mode:\n",
    "    # Utiliser un sous-ensemble du dataset pour le débogage\n",
    "    app_datas = app_datas.sample(frac=0.24, random_state=42)\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X = app_datas.drop(columns=['TARGET'])\n",
    "y = app_datas['TARGET']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Créer un pipeline avec imputation KNN, standardisation et SMOTE\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=3)),  # Imputation des valeurs manquantes\n",
    "    ('scaler', StandardScaler()),  # Standardisation des caractéristiques\n",
    "    ('smote', SMOTE(random_state=42))  # Équilibrage des classes avec SMOTE\n",
    "])\n",
    "\n",
    "# Appliquer le pipeline sur les données d'entraînement\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Transformer les données de test sans SMOTE\n",
    "X_test_imputed = pipeline.named_steps['imputer'].transform(X_test)\n",
    "X_test_scaled = pipeline.named_steps['scaler'].transform(X_test_imputed)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train_resampled.shape}\")\n",
    "print(f\"Shape of X_test: {X_test_scaled.shape}\")\n",
    "print(f\"Distribution in y_train: {pd.Series(y_train_resampled).value_counts(normalize=True)}\")\n",
    "print(f\"Distribution in y_test: {pd.Series(y_test).value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fdffa-8685-4e68-81f5-8ffa7531f3bc",
   "metadata": {},
   "source": [
    "# Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b8377-df59-40ef-9489-9158c89f7e96",
   "metadata": {},
   "source": [
    "## fonction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a7f78-da47-4d75-b0db-e6043bff5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "\n",
    "def evaluate_and_log_model(model, model_name, model_type, params, X_train, y_train, X_test, y_test):\n",
    "    with mlflow.start_run():\n",
    "        # Entraîner le modèle\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Prédictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Évaluer les performances\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        fp = conf_matrix[0][1]\n",
    "        fn = conf_matrix[1][0]\n",
    "        cost = fp + 10 * fn\n",
    "\n",
    "        # Logger les métriques et les hyperparamètres\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"Cost\", cost)\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        \n",
    "        # Enregistrer le modèle\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        \n",
    "        print(f\"{model_name} - AUC: {auc}\")\n",
    "        print(f\"{model_name} - Accuracy: {accuracy}\")\n",
    "        print(f\"{model_name} - Cost: {cost}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183294e-7bd9-48ee-bc0f-9d790f4dcee0",
   "metadata": {},
   "source": [
    "## Modèle Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a14bfe-b7b4-4338-b816-4b47f4190b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_params = {\"strategy\": \"most_frequent\"}\n",
    "dummy_clf = DummyClassifier(**dummy_params)\n",
    "evaluate_and_log_model(dummy_clf, \"dummy_classifier_model\", \"Dummy Classifier\", dummy_params, X_train_resampled, y_train_resampled, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd64759-ad51-470b-8b78-6e366cb134ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ddd36-c870-451c-8e7a-95caa5896189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8dd353a-b809-4a21-9586-e3eba7b72b52",
   "metadata": {},
   "source": [
    "## Modèle Regression lineaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d4f4a-2f0a-4299-b542-4b48fce1b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_params = {\n",
    "    \"penalty\": 'l2',\n",
    "    \"C\": 1.0,\n",
    "    \"class_weight\": 'balanced',\n",
    "    \"random_state\": 42,\n",
    "    \"max_iter\": 1000\n",
    "}\n",
    "log_reg = LogisticRegression(**log_reg_params)\n",
    "evaluate_and_log_model(log_reg, \"logistic_regression_model\", \"Logistic Regression\", log_reg_params, X_train_resampled, y_train_resampled, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82551c02-2cda-4ab9-9292-4db31a1386a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da659b3b-cd82-4a1d-8b85-73cc378b207b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f30c6e1b-c4e0-4012-825b-3e78965ffe48",
   "metadata": {},
   "source": [
    "## Modèle Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c854c5d-5ded-4f5b-a88a-89125b8e5ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"class_weight\": 'balanced',\n",
    "    \"random_state\": 42\n",
    "}\n",
    "rf_clf = RandomForestClassifier(**rf_params)\n",
    "evaluate_and_log_model(rf_clf, \"random_forest_model\", \"Random Forest\", rf_params, X_train_resampled, y_train_resampled, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb407721-a4d3-4bbf-9c02-456ee0bda9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d2ed3-2aba-4c60-b0dc-fa6676162dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8be32d8c-0e04-4366-b32c-f7e14d48a00f",
   "metadata": {},
   "source": [
    "## Modèle XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb939a20-eaff-4b15-8d74-5e17e721fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": 'binary:logistic',\n",
    "    \"random_state\": 42,\n",
    "    \"eval_metric\": 'auc'\n",
    "}\n",
    "xgb_clf = XGBClassifier(**xgb_params)\n",
    "evaluate_and_log_model(xgb_clf, \"xgboost_model\", \"XGBoost\", xgb_params, X_train_resampled, y_train_resampled, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c718e9-bebc-414f-ab1d-62fc5c3162a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de55d6af-192e-46d9-8498-b88764423cd7",
   "metadata": {},
   "source": [
    "## création du score personnalisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b9f6e-ffc1-4619-aadb-62ec6ea0f412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339600d9-6c5c-4764-8931-e7155e1cc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "\n",
    "# Définir une fonction pour calculer le coût\n",
    "def custom_cost(y_true, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    fp = conf_matrix[0][1]\n",
    "    fn = conf_matrix[1][0]\n",
    "    cost = fp + 10 * fn\n",
    "    return cost\n",
    "\n",
    "# Créer un make_scorer à partir de la fonction custom_cost\n",
    "cost_scorer = make_scorer(custom_cost, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d1675-f381-48dd-bf6b-cc4b27f502a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eddae018-46a1-4339-a1d4-1b06ef856bcf",
   "metadata": {},
   "source": [
    "## Gridsearchcv sur XGBOost et log mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4808bf-0984-42e5-b129-015fab142689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e1e0f-5fd0-4321-b54a-1a07beb1d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Définir les hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Créer le modèle XGBoost\n",
    "xgb_clf = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Configurer GridSearchCV avec le score personnalisé\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring=cost_scorer, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fonction pour évaluer et enregistrer les résultats de GridSearchCV dans MLflow\n",
    "def log_grid_search_cv_results(grid_search, X_train, y_train, X_test, y_test):\n",
    "    with mlflow.start_run():\n",
    "        # Loguer le type de modèle\n",
    "        mlflow.log_param(\"model_type\", \"XGBoost with GridSearchCV\")\n",
    "        \n",
    "        # Entraîner le GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Logger tous les résultats de GridSearchCV\n",
    "        cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "        for i in range(len(cv_results)):\n",
    "            for key in cv_results.columns:\n",
    "                value = cv_results[key].iloc[i]\n",
    "                if isinstance(value, (float, int, np.number)):\n",
    "                    mlflow.log_metric(f\"{key}_{i}\", value)\n",
    "                elif isinstance(value, (list, dict)):\n",
    "                    # Convert lists or dicts to a string representation\n",
    "                    mlflow.log_param(f\"{key}_{i}\", str(value))\n",
    "        \n",
    "        # Enregistrer les meilleurs hyperparamètres\n",
    "        best_params = grid_search.best_params_\n",
    "        mlflow.log_params(best_params)\n",
    "        \n",
    "        # Afficher les meilleurs hyperparamètres\n",
    "        print(\"Best Hyperparameters:\", best_params)\n",
    "        \n",
    "        # Enregistrer le meilleur modèle\n",
    "        best_model = grid_search.best_estimator_\n",
    "        mlflow.sklearn.log_model(best_model, \"best_xgboost_model\")\n",
    "        \n",
    "        # Prédictions avec le meilleur modèle\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Évaluer les performances\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        fp = conf_matrix[0][1]\n",
    "        fn = conf_matrix[1][0]\n",
    "        cost = fp + 10 * fn\n",
    "        \n",
    "        # Logger les métriques\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"Cost\", cost)\n",
    "        \n",
    "        print(f\"Best Model - AUC: {auc}\")\n",
    "        print(f\"Best Model - Accuracy: {accuracy}\")\n",
    "        print(f\"Best Model - Cost: {cost}\")\n",
    "\n",
    "# Exécuter la fonction pour loguer les résultats de GridSearchCV\n",
    "log_grid_search_cv_results(grid_search, X_train_resampled, y_train_resampled, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d707e-5165-40a7-96fd-6a0b0c460b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af4290b4-f4ca-46e4-a895-4f4a9903cc8f",
   "metadata": {},
   "source": [
    "## descriptions features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c581fb-18b4-4362-ba26-8d1add5b4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier de description des colonnes avec l'encodage correct\n",
    "try:\n",
    "    columns_description = pd.read_csv('../csv_files/HomeCredit_columns_description.csv', encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    columns_description = pd.read_csv('../csv_files/HomeCredit_columns_description.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Filtrer pour obtenir uniquement les descriptions pertinentes pour les caractéristiques utilisées dans l'application {train|test}\n",
    "feature_descriptions = columns_description[columns_description['Table'].str.contains('application_{train|test}.csv', na=False)]\n",
    "feature_descriptions = feature_descriptions[['Row', 'Description']].set_index('Row')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f0670-64f9-4744-a336-80aa4b670415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99088fe7-61d1-4268-8127-7010c9cef19b",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec1290-4ba0-4bdb-a579-d7b9a4286f65",
   "metadata": {},
   "source": [
    "## globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11822c7-154e-449b-b583-4756df0e1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "# Entraîner le meilleur modèle XGBoost avec les meilleurs hyperparamètres trouvés\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Extraire l'importance des caractéristiques\n",
    "importance = best_model.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# Convertir en DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': list(importance.keys()),\n",
    "    'Importance': list(importance.values())\n",
    "})\n",
    "\n",
    "# Assigner des noms significatifs aux features basées sur leur ordre dans les données d'origine\n",
    "feature_names = X_train.columns\n",
    "importance_df['Feature'] = importance_df['Feature'].apply(lambda x: feature_names[int(x[1:])])\n",
    "\n",
    "# Ajouter les descriptions des caractéristiques\n",
    "importance_df['Description'] = importance_df['Feature'].map(feature_descriptions['Description'])\n",
    "\n",
    "# Remplacer les valeurs NaN dans les descriptions par une chaîne vide pour éviter les erreurs\n",
    "importance_df['Description'] = importance_df['Description'].fillna('')\n",
    "\n",
    "# Trier les caractéristiques par importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Calculer le pourcentage d'importance\n",
    "importance_df['Importance'] = 100 * (importance_df['Importance'] / importance_df['Importance'].sum())\n",
    "\n",
    "# Afficher les 20 premières caractéristiques\n",
    "top_20_features = importance_df.head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Ajustez la taille de la figure pour plus de lisibilité\n",
    "plt.barh(top_20_features['Feature'], top_20_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance (%)', fontsize=14)\n",
    "plt.title('Top 20 Feature Importance (XGBoost)', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "\n",
    "# Afficher les descriptions des 20 premières caractéristiques\n",
    "descriptions = [f\"{feature}: {desc}\" for feature, desc in zip(top_20_features['Feature'], top_20_features['Description'])]\n",
    "\n",
    "# Utiliser print pour afficher les descriptions\n",
    "for description in descriptions:\n",
    "    print(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe697e-d38f-4562-93cc-1b16bbd711d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e82ed6-fecf-4580-8a0c-2e69af08c24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6ab5b-0de0-4082-8f5a-e15910183a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89a582-3da1-4b28-908d-3d012c3e23cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10990df-3b7a-431e-a285-089a8d89725e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011cf96-85be-4d9b-b3f4-74adc62a18d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c7fe681-69bd-4574-950e-fa995a7fc48a",
   "metadata": {},
   "source": [
    "## locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c0ef2-933c-41a1-a030-a2d413c04f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Créer un explainer SHAP pour le modèle XGBoost\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Sélectionner un client avec une cible de 0 et un client avec une cible de 1\n",
    "indices_target_0 = y_test[y_test == 0].index\n",
    "indices_target_1 = y_test[y_test == 1].index\n",
    "\n",
    "# Vérifier que les indices existent\n",
    "if len(indices_target_0) > 0:\n",
    "    index_target_0 = indices_target_0[0]\n",
    "    # Convertir l'index global en index relatif\n",
    "    relative_index_target_0 = y_test.index.get_loc(index_target_0)\n",
    "    X_target_0 = X_test_scaled[relative_index_target_0].reshape(1, -1)\n",
    "    shap_values_target_0 = explainer.shap_values(X_target_0)\n",
    "    \n",
    "    # Afficher l'analyse SHAP pour un client avec une cible de 0\n",
    "    print(\"Crédit accepté\")\n",
    "    shap.initjs()\n",
    "    shap.force_plot(explainer.expected_value, shap_values_target_0, X_target_0, feature_names=feature_names, matplotlib=True)\n",
    "\n",
    "if len(indices_target_1) > 0:\n",
    "    index_target_1 = indices_target_1[0]\n",
    "    # Convertir l'index global en index relatif\n",
    "    relative_index_target_1 = y_test.index.get_loc(index_target_1)\n",
    "    X_target_1 = X_test_scaled[relative_index_target_1].reshape(1, -1)\n",
    "    shap_values_target_1 = explainer.shap_values(X_target_1)\n",
    "    \n",
    "    # Afficher l'analyse SHAP pour un client avec une cible de 1\n",
    "    print(\"Crédit refusé\")\n",
    "    shap.initjs()\n",
    "    shap.force_plot(explainer.expected_value, shap_values_target_1, X_target_1, feature_names=feature_names, matplotlib=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab531c4-7903-4516-a974-ee68055aa20a",
   "metadata": {},
   "source": [
    "# temps de fin d'execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d8969-7e72-40a1-b99f-4645f6a62af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marquer la fin de l'exécution\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer le temps d'exécution total\n",
    "execution_time = end_time - start_time\n",
    "execution_time_str = str(datetime.timedelta(seconds=execution_time))\n",
    "\n",
    "print(f\"Exécution du notebook terminée à: {datetime.datetime.now()}\")\n",
    "print(f\"Temps d'exécution total: {execution_time_str}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
